"""
PySpark Code Generator
Converts SnapLogic snaps to Databricks PySpark code.
"""
from typing import Dict, List, Any, Tuple

class SparkGenerator:
    """Generates PySpark code from SnapLogic pipeline structure."""
    
    # Mapping of SnapLogic snap types to generator methods
    SNAP_HANDLERS = {
        # Read Snaps
        "csvreader": "_gen_csv_reader",
        "jsonreader": "_gen_json_reader",
        "xmlreader": "_gen_xml_reader",
        "parquetreader": "_gen_parquet_reader",
        "filereader": "_gen_file_reader",
        "dbreader": "_gen_db_reader",
        "jdbcreader": "_gen_db_reader",
        "oraclereader": "_gen_db_reader",
        "sqlserverreader": "_gen_db_reader",
        "postgresreader": "_gen_db_reader",
        "mysqlreader": "_gen_db_reader",
        "s3reader": "_gen_s3_reader",
        "azureblobreader": "_gen_azure_reader",
        
        # Write Snaps
        "csvwriter": "_gen_csv_writer",
        "jsonwriter": "_gen_json_writer",
        "parquetwriter": "_gen_parquet_writer",
        "filewriter": "_gen_file_writer",
        "dbwriter": "_gen_db_writer",
        "jdbcwriter": "_gen_db_writer",
        "s3writer": "_gen_s3_writer",
        "azureblobwriter": "_gen_azure_writer",
        
        # Transform Snaps
        "mapper": "_gen_mapper",
        "filter": "_gen_filter",
        "sort": "_gen_sort",
        "aggregate": "_gen_aggregate",
        "join": "_gen_join",
        "union": "_gen_union",
        "router": "_gen_router",
        "groupby": "_gen_groupby",
        "distinct": "_gen_distinct",
        "limit": "_gen_limit",
        "head": "_gen_limit",
        "tail": "_gen_tail",
        "sample": "_gen_sample",
        
        # Script Snaps
        "script": "_gen_script",
        "python": "_gen_python",
        "expression": "_gen_expression",
        
        # Flow Control
        "pipelineexecute": "_gen_pipeline_execute",
        "gate": "_gen_gate",
        "merge": "_gen_merge",
        "copy": "_gen_copy",
    }
    
    def __init__(self):
        self.variable_counter = 0
        self.generated_vars = {}
    
    def generate(self, pipeline_data: Dict[str, Any]) -> Tuple[str, List[Dict]]:
        """
        Generate PySpark code from pipeline data.
        
        Args:
            pipeline_data: Parsed pipeline structure
            
        Returns:
            Tuple of (generated_code, list_of_unknown_snaps)
        """
        self.variable_counter = 0
        self.generated_vars = {}
        
        code_lines = []
        unknown_snaps = []
        
        # Header
        pipeline_name = pipeline_data.get("name", "Converted Pipeline")
        code_lines.extend([
            "# Databricks notebook source",
            f"# Converted from SnapLogic Pipeline: {pipeline_name}",
            "# Auto-generated by SnapLogic to Databricks Converter",
            "",
            "# COMMAND ----------",
            "",
            "# Import required libraries",
            "from pyspark.sql import SparkSession",
            "from pyspark.sql import functions as F",
            "from pyspark.sql.types import *",
            "from pyspark.sql.window import Window",
            "",
            "# COMMAND ----------",
            "",
            "# Initialize Spark session (already available in Databricks)",
            "spark = SparkSession.builder.getOrCreate()",
            "",
            "# COMMAND ----------",
            ""
        ])
        
        # Process each snap
        snaps = pipeline_data.get("snaps", [])
        for snap in snaps:
            snap_code, is_unknown = self._generate_snap_code(snap)
            
            if is_unknown:
                unknown_snaps.append(snap)
            
            code_lines.append(f"# --- {snap.get('name', snap.get('id', 'Unknown'))} ---")
            code_lines.append(snap_code)
            code_lines.append("")
            code_lines.append("# COMMAND ----------")
            code_lines.append("")
        
        # Footer
        code_lines.extend([
            "# Pipeline execution complete",
            "print('Pipeline execution completed successfully')"
        ])
        
        return "\n".join(code_lines), unknown_snaps
    
    def _generate_snap_code(self, snap: Dict) -> Tuple[str, bool]:
        """Generate code for a single snap."""
        snap_type = snap.get("type", "").lower()
        
        # Clean up snap type for matching
        snap_type_clean = snap_type.replace("com-snaplogic-snaps-", "").replace("com.snaplogic.snaps.", "")
        snap_type_clean = snap_type_clean.replace("transform-", "").replace("read-", "").replace("write-", "")
        snap_type_clean = snap_type_clean.replace("snap", "").replace("-", "").replace(".", "")
        
        # Find handler
        handler_name = None
        for key, handler in self.SNAP_HANDLERS.items():
            if key in snap_type_clean or snap_type_clean in key:
                handler_name = handler
                break
        
        if handler_name and hasattr(self, handler_name):
            handler = getattr(self, handler_name)
            return handler(snap), False
        else:
            return self._gen_unknown(snap), True
    
    def _get_var_name(self, base: str = "df") -> str:
        """Generate a unique variable name."""
        self.variable_counter += 1
        return f"{base}_{self.variable_counter}"
    
    def _get_property(self, snap: Dict, *keys: str, default: str = "") -> str:
        """Get a property value from snap, checking multiple possible keys."""
        props = snap.get("properties", {})
        for key in keys:
            if key in props:
                value = props[key]
                if isinstance(value, dict) and "value" in value:
                    return value["value"]
                return value
        return default
    
    # ==================== READ SNAPS ====================
    
    def _gen_csv_reader(self, snap: Dict) -> str:
        var = self._get_var_name("df")
        path = self._get_property(snap, "file", "path", "File", default="/path/to/file.csv")
        header = self._get_property(snap, "hasHeader", "header", default="true")
        delimiter = self._get_property(snap, "delimiter", "separator", default=",")
        
        return f'''{var} = spark.read.format("csv") \\
    .option("header", "{header}") \\
    .option("delimiter", "{delimiter}") \\
    .option("inferSchema", "true") \\
    .load("{path}")

{var}.display()'''
    
    def _gen_json_reader(self, snap: Dict) -> str:
        var = self._get_var_name("df")
        path = self._get_property(snap, "file", "path", "File", default="/path/to/file.json")
        
        return f'''{var} = spark.read.format("json") \\
    .option("multiLine", "true") \\
    .load("{path}")

{var}.display()'''
    
    def _gen_parquet_reader(self, snap: Dict) -> str:
        var = self._get_var_name("df")
        path = self._get_property(snap, "file", "path", "File", default="/path/to/file.parquet")
        
        return f'''{var} = spark.read.parquet("{path}")

{var}.display()'''
    
    def _gen_xml_reader(self, snap: Dict) -> str:
        var = self._get_var_name("df")
        path = self._get_property(snap, "file", "path", default="/path/to/file.xml")
        row_tag = self._get_property(snap, "rowTag", "row_tag", default="row")
        
        return f'''{var} = spark.read.format("xml") \\
    .option("rowTag", "{row_tag}") \\
    .load("{path}")

{var}.display()'''
    
    def _gen_file_reader(self, snap: Dict) -> str:
        var = self._get_var_name("df")
        path = self._get_property(snap, "file", "path", default="/path/to/file")
        
        return f'''{var} = spark.read.text("{path}")

{var}.display()'''
    
    def _gen_db_reader(self, snap: Dict) -> str:
        var = self._get_var_name("df")
        table = self._get_property(snap, "table", "tableName", default="table_name")
        url = self._get_property(snap, "url", "connectionUrl", default="jdbc:database://host:port/db")
        
        return f'''{var} = spark.read.format("jdbc") \\
    .option("url", "{url}") \\
    .option("dbtable", "{table}") \\
    .option("user", dbutils.secrets.get(scope="db", key="username")) \\
    .option("password", dbutils.secrets.get(scope="db", key="password")) \\
    .load()

{var}.display()'''
    
    def _gen_s3_reader(self, snap: Dict) -> str:
        var = self._get_var_name("df")
        path = self._get_property(snap, "bucket", "s3Path", default="s3://bucket/path")
        file_format = self._get_property(snap, "format", "fileType", default="parquet")
        
        return f'''{var} = spark.read.format("{file_format}") \\
    .load("{path}")

{var}.display()'''
    
    def _gen_azure_reader(self, snap: Dict) -> str:
        var = self._get_var_name("df")
        container = self._get_property(snap, "container", default="container")
        path = self._get_property(snap, "path", "blobPath", default="path/to/file")
        
        return f'''{var} = spark.read.format("parquet") \\
    .load(f"abfss://{container}@{{storage_account}}.dfs.core.windows.net/{path}")

{var}.display()'''
    
    # ==================== WRITE SNAPS ====================
    
    def _gen_csv_writer(self, snap: Dict) -> str:
        path = self._get_property(snap, "file", "path", default="/path/to/output.csv")
        mode = self._get_property(snap, "writeMode", "mode", default="overwrite")
        
        return f'''df.write.format("csv") \\
    .mode("{mode}") \\
    .option("header", "true") \\
    .save("{path}")'''
    
    def _gen_json_writer(self, snap: Dict) -> str:
        path = self._get_property(snap, "file", "path", default="/path/to/output.json")
        mode = self._get_property(snap, "writeMode", "mode", default="overwrite")
        
        return f'''df.write.format("json") \\
    .mode("{mode}") \\
    .save("{path}")'''
    
    def _gen_parquet_writer(self, snap: Dict) -> str:
        path = self._get_property(snap, "file", "path", default="/path/to/output.parquet")
        mode = self._get_property(snap, "writeMode", "mode", default="overwrite")
        
        return f'''df.write.format("parquet") \\
    .mode("{mode}") \\
    .save("{path}")'''
    
    def _gen_file_writer(self, snap: Dict) -> str:
        path = self._get_property(snap, "file", "path", default="/path/to/output")
        mode = self._get_property(snap, "writeMode", "mode", default="overwrite")
        
        return f'''df.write.format("text") \\
    .mode("{mode}") \\
    .save("{path}")'''
    
    def _gen_db_writer(self, snap: Dict) -> str:
        table = self._get_property(snap, "table", "tableName", default="output_table")
        mode = self._get_property(snap, "writeMode", "mode", default="overwrite")
        url = self._get_property(snap, "url", "connectionUrl", default="jdbc:database://host:port/db")
        
        return f'''df.write.format("jdbc") \\
    .option("url", "{url}") \\
    .option("dbtable", "{table}") \\
    .option("user", dbutils.secrets.get(scope="db", key="username")) \\
    .option("password", dbutils.secrets.get(scope="db", key="password")) \\
    .mode("{mode}") \\
    .save()'''
    
    def _gen_s3_writer(self, snap: Dict) -> str:
        path = self._get_property(snap, "bucket", "s3Path", default="s3://bucket/output")
        mode = self._get_property(snap, "writeMode", "mode", default="overwrite")
        file_format = self._get_property(snap, "format", "fileType", default="parquet")
        
        return f'''df.write.format("{file_format}") \\
    .mode("{mode}") \\
    .save("{path}")'''
    
    def _gen_azure_writer(self, snap: Dict) -> str:
        container = self._get_property(snap, "container", default="container")
        path = self._get_property(snap, "path", "blobPath", default="output/path")
        mode = self._get_property(snap, "writeMode", "mode", default="overwrite")
        
        return f'''df.write.format("parquet") \\
    .mode("{mode}") \\
    .save(f"abfss://{container}@{{storage_account}}.dfs.core.windows.net/{path}")'''
    
    # ==================== TRANSFORM SNAPS ====================
    
    def _gen_mapper(self, snap: Dict) -> str:
        var = self._get_var_name("df_mapped")
        mappings = self._get_property(snap, "mappings", "expressions", "mapping", default=[])
        
        if isinstance(mappings, list) and mappings:
            select_exprs = []
            for m in mappings[:10]:  # Limit to first 10 for readability
                if isinstance(m, dict):
                    target = m.get("target", m.get("output", "column"))
                    expr = m.get("expression", m.get("value", "col"))
                    select_exprs.append(f'    F.expr("{expr}").alias("{target}")')
            
            if select_exprs:
                newline = "\n"
                return f'''{var} = df.select(
{(","+newline).join(select_exprs)}
)'''
        
        return f'''{var} = df.select(
    # TODO: Add column mappings from SnapLogic Mapper configuration
    "*"  # Placeholder - review original mapping
)'''
    
    def _gen_filter(self, snap: Dict) -> str:
        var = self._get_var_name("df_filtered")
        condition = self._get_property(snap, "condition", "expression", "filter", default="true")
        
        return f'''{var} = df.filter(F.expr("{condition}"))'''
    
    def _gen_sort(self, snap: Dict) -> str:
        var = self._get_var_name("df_sorted")
        columns = self._get_property(snap, "sortColumns", "columns", "orderBy", default=[])
        
        if isinstance(columns, list) and columns:
            sort_cols = ", ".join([f'F.col("{c}").asc()' for c in columns[:5]])
            return f'''{var} = df.orderBy({sort_cols})'''
        
        return f'''{var} = df.orderBy(
    # TODO: Specify sort columns
    F.col("column_name").asc()
)'''
    
    def _gen_aggregate(self, snap: Dict) -> str:
        var = self._get_var_name("df_agg")
        group_by = self._get_property(snap, "groupBy", "groupByFields", default=[])
        aggs = self._get_property(snap, "aggregations", "functions", default=[])
        
        return f'''{var} = df.groupBy(
    # TODO: Specify grouping columns from SnapLogic config
    "group_column"
).agg(
    # TODO: Specify aggregation functions
    F.sum("value_column").alias("total"),
    F.count("*").alias("count")
)'''
    
    def _gen_join(self, snap: Dict) -> str:
        var = self._get_var_name("df_joined")
        join_type = self._get_property(snap, "joinType", "type", default="inner")
        join_condition = self._get_property(snap, "condition", "joinCondition", default="")
        
        return f'''{var} = df_left.join(
    df_right,
    on=df_left["key"] == df_right["key"],  # TODO: Adjust join condition
    how="{join_type.lower()}"
)'''
    
    def _gen_union(self, snap: Dict) -> str:
        var = self._get_var_name("df_union")
        
        return f'''{var} = df1.unionByName(df2, allowMissingColumns=True)'''
    
    def _gen_router(self, snap: Dict) -> str:
        routes = self._get_property(snap, "routes", "conditions", default=[])
        
        code = '''# Router - Split data based on conditions
# Each route creates a separate DataFrame\n'''
        
        for i, route in enumerate(routes[:5] if isinstance(routes, list) else []):
            condition = route.get("condition", "true") if isinstance(route, dict) else "true"
            code += f'''df_route_{i+1} = df.filter(F.expr("{condition}"))\n'''
        
        if not routes:
            code += '''df_route_1 = df.filter(F.expr("condition_1"))
df_route_2 = df.filter(F.expr("condition_2"))'''
        
        return code
    
    def _gen_groupby(self, snap: Dict) -> str:
        return self._gen_aggregate(snap)
    
    def _gen_distinct(self, snap: Dict) -> str:
        var = self._get_var_name("df_distinct")
        return f'''{var} = df.distinct()'''
    
    def _gen_limit(self, snap: Dict) -> str:
        var = self._get_var_name("df_limited")
        limit = self._get_property(snap, "limit", "count", "rows", default="100")
        return f'''{var} = df.limit({limit})'''
    
    def _gen_tail(self, snap: Dict) -> str:
        var = self._get_var_name("df_tail")
        limit = self._get_property(snap, "limit", "count", default="100")
        return f'''{var} = spark.createDataFrame(df.tail({limit}), df.schema)'''
    
    def _gen_sample(self, snap: Dict) -> str:
        var = self._get_var_name("df_sample")
        fraction = self._get_property(snap, "fraction", "percentage", default="0.1")
        return f'''{var} = df.sample(fraction={fraction})'''
    
    # ==================== SCRIPT SNAPS ====================
    
    def _gen_script(self, snap: Dict) -> str:
        script = self._get_property(snap, "script", "code", default="")
        
        return f'''# Script Snap - Original script may need adaptation
# Original SnapLogic script:
"""
{script}
"""
# TODO: Convert the above script to PySpark/Python'''
    
    def _gen_python(self, snap: Dict) -> str:
        return self._gen_script(snap)
    
    def _gen_expression(self, snap: Dict) -> str:
        var = self._get_var_name("df_expr")
        expr = self._get_property(snap, "expression", "code", default="")
        
        return f'''{var} = df.withColumn(
    "result",
    F.expr("{expr}")  # TODO: Verify expression syntax
)'''
    
    # ==================== FLOW CONTROL ====================
    
    def _gen_pipeline_execute(self, snap: Dict) -> str:
        pipeline_path = self._get_property(snap, "pipeline_path", "pipelinePath", default="Child_Pipeline")
        
        return f'''# Pipeline Execute - Calls child pipeline: {pipeline_path}
# In Databricks, this can be implemented as:
# Option 1: %run "./child_notebook"
# Option 2: dbutils.notebook.run("./child_notebook", timeout_seconds=600, arguments={{}})

result = dbutils.notebook.run(
    "{pipeline_path}",
    timeout_seconds=600,
    arguments={{}}
)'''
    
    def _gen_gate(self, snap: Dict) -> str:
        condition = self._get_property(snap, "condition", "expression", default="true")
        
        return f'''# Gate Snap - Conditional flow control
if {condition}:
    # Continue processing
    pass
else:
    # Stop or skip
    dbutils.notebook.exit("Gate condition not met")'''
    
    def _gen_merge(self, snap: Dict) -> str:
        var = self._get_var_name("df_merged")
        
        return f'''{var} = df1.unionByName(df2, allowMissingColumns=True)'''
    
    def _gen_copy(self, snap: Dict) -> str:
        return '''# Copy Snap - Creates duplicate of the data
df_copy = df'''
    
    # ==================== UNKNOWN SNAPS ====================
    
    def _gen_unknown(self, snap: Dict) -> str:
        snap_type = snap.get("type", "Unknown")
        snap_name = snap.get("name", snap.get("id", "Unknown"))
        
        return f'''# TODO: Unknown snap - {snap_type}
# Snap Name: {snap_name}
# This snap type was not recognized and requires manual conversion.
# Original snap properties:
# {snap.get("properties", {})}
# 
# Please review the SnapLogic documentation and implement equivalent PySpark logic.
# You can also use the "Ask AI" feature to get suggestions.'''
